from langchain.memory import ConversationSummaryBufferMemory, ConversationSummaryMemory
from langchain.llms.bedrock import Bedrock
from langchain.chains import ConversationChain
from langchain.chains import RetrievalQA
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate


import streamlit as st
from sidebar_app import update_input_tokens

model_kwargs = { 
    "max_tokens_to_sample": 1024, 
    "temperature": 1, 
    "top_p": 0.9, 
    "stop_sequences": ["Human:"]
}

default_model_id = "anthropic.claude-instant-v1"

# Clase para llevar la cuenta de los tokens utilizados por el modelo


def get_llm(streaming_callback=None, invocation_kwargs=None, model_id=None):
    
    this_model_id = model_id if model_id else default_model_id # si viene un model_id se usa ese, caso contrario se usa el default
    bedrock_base_kwargs = dict(model_id=default_model_id, model_kwargs= model_kwargs)
    
    if invocation_kwargs: # en el caso de hacer override de invocation_kwargs tales como temperatura y max tokens
        bedrock_base_kwargs = dict(model_id=this_model_id, model_kwargs= {**model_kwargs, **invocation_kwargs})

    new_kwargs = dict(**bedrock_base_kwargs)

    if streaming_callback: # la respuesta tiene que ser en streamin?, pasamos streaming_callback al modelo
        new_kwargs = dict(**bedrock_base_kwargs, streaming=True,callbacks=[streaming_callback])

    #print("new_kwargs:",new_kwargs)

    llm = Bedrock(**new_kwargs)
    
    return llm


def get_memory(): 
    
    # ConversationSummaryBufferMemory requiere un LLM para resumir los mensajes viejos
    # Permite mantener la idea sin extender m√°s alla de limite

    llm = get_llm()
    
    memory1 = ConversationSummaryBufferMemory(
        llm=llm, max_token_limit=512,
        memory_key="chat_history",
        return_messages=True,
        human_prefix = "H", ai_prefix= "A" 
    ) 
    memory2 = ConversationSummaryMemory(
        return_messages=True,
        llm=llm, max_token_limit=512, ai_prefix="A", human_prefix="H", memory_key="chat_history")
    
    return memory1


def get_chat_response(prompt, memory, streaming_callback=None,invocation_kwargs=None, model_id= None, input_token_placeholder=None):
    
    llm = get_llm(streaming_callback, invocation_kwargs, model_id) 
    


    
    base_prompt = """The following is a friendly conversation between a human and an AI. 
    The AI is talkative and provides lots of specific details from its context. 
    If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:
    {chat_history}

    Human:{input}

    Assistant:"""

    BASE_PROMPT_TEMPLATE = PromptTemplate(input_variables=["chat_history", "question"],template=base_prompt)

    conversation_with_summary = ConversationChain( #create a chat client
        llm = llm, #using the Bedrock LLM
        prompt= BASE_PROMPT_TEMPLATE,
        memory = memory, #with the summarization memory
        verbose = True #print out some of the internal states of the chain while running
    )


    inputs_pre = conversation_with_summary.prep_inputs({"input": prompt})
    print (inputs_pre)
    final_input  =  conversation_with_summary.prompt.template.format(chat_history=inputs_pre['chat_history'], input=inputs_pre['input'])

    tc = st.session_state.token_counter
    tc.new_input(user_input=prompt, all_input=final_input)
    update_input_tokens(tc.user_input_tokens, tc.all_input_tokens, tc.total_input_tokens, input_token_placeholder )

    return conversation_with_summary.predict(input=prompt)


def get_chat_rag_response(prompt, retriever, memory, streaming_callback=None,invocation_kwargs=None, model_id= None, input_token_placeholder=None):
    
    llm = get_llm(streaming_callback, invocation_kwargs, model_id) 


    qa = ConversationalRetrievalChain.from_llm(
        llm,

        verbose=True,
        retriever=retriever,
        memory=memory,
        get_chat_history=lambda h : h
    ) 


    return qa.run({"question": prompt})



def test_vector_db(prompt, vector_db, streaming_callback=None):
    llm = get_llm(streaming_callback) 
    retriever= vector_db.as_retriever(search_type = "mmr",  search_kwargs={"k": 5})

    qa_chain = RetrievalQA.from_chain_type( llm, verbose=True, retriever=retriever)

    
    return qa_chain.run({"query": prompt, })
